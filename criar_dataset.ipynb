{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from IPython.display import display\n",
    "\n",
    "BASE_PAYLOAD = {\n",
    "    \"parentQid\": \"8e331f13-5696-41dd-8362-22da1d43bed7-017595e076\",\n",
    "    \"sortBy\": \"times-cited-descending\",\n",
    "    \"displayTimesCited\": \"true\",\n",
    "    \"displayCitedRefs\": \"true\",\n",
    "    \"product\": \"UA\",\n",
    "    \"colName\": \"WOS\",\n",
    "    \"fileOpt\": \"xls\",\n",
    "    \"action\": \"saveToExcel\",\n",
    "    \"view\": \"summary\",\n",
    "    \"isRefQuery\": \"false\",\n",
    "    \"locale\": \"en_US\",\n",
    "    \"filters\": \"fullRecord\"\n",
    "}\n",
    "\n",
    "def read_data():\n",
    "\n",
    "    total_records = 12000\n",
    "    page_size = 1000\n",
    "\n",
    "    # O loop vai de 0 a 11000, com passos de 1000.\n",
    "    for i in range(9000, total_records, page_size):\n",
    "        start_record = i + 1\n",
    "        end_record = i + page_size\n",
    "\n",
    "        print(f\"Baixando registros de {start_record} a {end_record}...\")\n",
    "\n",
    "        payload = BASE_PAYLOAD.copy()\n",
    "        payload[\"markFrom\"] = str(start_record)\n",
    "        payload[\"markTo\"] = str(end_record)\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                URL,\n",
    "                headers=HEADERS,\n",
    "                cookies=COOKIES,\n",
    "                json=payload  # 'json=' serializa o dict para JSON e define o Content-Type\n",
    "            )\n",
    "\n",
    "            response.raise_for_status()\n",
    "\n",
    "            filename = f\"export_{start_record}_to_{end_record}.xls\"\n",
    "\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            print(f\"Arquivo '{filename}' salvo com sucesso!\")\n",
    "\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            print(f\"Erro HTTP: {http_err} - Resposta: {response.text}\")\n",
    "            break\n",
    "        except Exception as err:\n",
    "            print(f\"Ocorreu um erro: {err}\")\n",
    "            break #\n",
    "\n",
    "        print(\"Aguardando 2 segundos...\\n\")\n",
    "        time.sleep(2)\n",
    "\n",
    "    print(\"Processo de exportação concluído!\")\n",
    "\n",
    "read_data()"
   ],
   "id": "bd5db28e4df5e8d3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T20:58:25.048135Z",
     "start_time": "2025-08-24T20:58:21.142208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "file_pattern = \"exprot*\"\n",
    "\n",
    "xls_files = glob.glob(pathname=\"datasets/*\")\n",
    "\n",
    "if not xls_files:\n",
    "    print(\"Nenhum arquiv\")\n",
    "else:\n",
    "    xls_files.sort()\n",
    "    print(f\"📄 Encontrados {len(xls_files)} arquivos:\")\n",
    "    for f in xls_files:\n",
    "        print(f\"  - {f}\")\n",
    "dataframes_list = []\n",
    "\n",
    "for file in xls_files:\n",
    "    try:\n",
    "        print(f\"Lendo o arquivo: {file}...\")\n",
    "        df = pd.read_excel(file)\n",
    "        dataframes_list.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo {file}: {e}\")\n",
    "\n",
    "print(\"\\n✅ Processo de leitura concluído!\")\n",
    "if dataframes_list:\n",
    "    dados_completos_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "    print(\"DataFrames combinados\")\n",
    "else:\n",
    "    print(\"Nenhum dado\")\n",
    "\n",
    "output_csv_file = 'datasets/dados_completos.csv'\n",
    "dados_completos_df.to_csv(output_csv_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Arquivo '{output_csv_file}' salvo\")"
   ],
   "id": "5b548511afe6b90c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Encontrados 12 arquivos:\n",
      "  - datasets\\export_10001_to_11000.xls\n",
      "  - datasets\\export_1001_to_2000.xls\n",
      "  - datasets\\export_11001_to_12000.xls\n",
      "  - datasets\\export_1_to_1000.xls\n",
      "  - datasets\\export_2001_to_3000.xls\n",
      "  - datasets\\export_3001_to_4000.xls\n",
      "  - datasets\\export_4001_to_5000.xls\n",
      "  - datasets\\export_5001_to_6000.xls\n",
      "  - datasets\\export_6001_to_7000.xls\n",
      "  - datasets\\export_7001_to_8000.xls\n",
      "  - datasets\\export_8001_to_9000.xls\n",
      "  - datasets\\export_9001_to_10000.xls\n",
      "Lendo o arquivo: datasets\\export_10001_to_11000.xls...\n",
      "Lendo o arquivo: datasets\\export_1001_to_2000.xls...\n",
      "Lendo o arquivo: datasets\\export_11001_to_12000.xls...\n",
      "Lendo o arquivo: datasets\\export_1_to_1000.xls...\n",
      "Lendo o arquivo: datasets\\export_2001_to_3000.xls...\n",
      "Lendo o arquivo: datasets\\export_3001_to_4000.xls...\n",
      "Lendo o arquivo: datasets\\export_4001_to_5000.xls...\n",
      "Lendo o arquivo: datasets\\export_5001_to_6000.xls...\n",
      "Lendo o arquivo: datasets\\export_6001_to_7000.xls...\n",
      "Lendo o arquivo: datasets\\export_7001_to_8000.xls...\n",
      "Lendo o arquivo: datasets\\export_8001_to_9000.xls...\n",
      "Lendo o arquivo: datasets\\export_9001_to_10000.xls...\n",
      "\n",
      "✅ Processo de leitura concluído!\n",
      "DataFrames combinados\n",
      "✅ Arquivo 'datasets/dados_completos.csv' salvo\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T20:46:49.570106Z",
     "start_time": "2025-08-24T20:46:49.362365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "URL = \"https://www-webofscience-com.ez130.periodicos.capes.gov.br/api/wosnx/indic/export/saveToFile\"\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "    'Cache-Control': 'no-cache',\n",
    "    'Content-Type': 'application/json',\n",
    "    'Origin': 'https://www-webofscience-com.ez130.periodicos.capes.gov.br',\n",
    "    'Pragma': 'no-cache',\n",
    "    'Referer': 'https://www-webofscience-com.ez130.periodicos.capes.gov.br/wos/woscc/summary/8e331f13-5696-41dd-8362-22da1d43bed7-017595e076/times-cited-descending/1(overlay:export/exc)',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',\n",
    "    'X-1P-WOS-SID': 'USW2EC0FB4rqqc0WHfHgnRmXfTYyb',\n",
    "}\n",
    "\n",
    "COOKIES = {\n",
    "    '_shibsession_64656661756c7468747470733a2f2f7777772e706572696f6469636f732e63617065732e676f762e62722f73686962626f6c657468': '_e5fe66b8784c2d61f83ea03aac56d9f3',\n",
    "    'ezproxy': 'YVKVfWdHBs0qTTVcgSwdgDzvgYQsoo2',\n",
    "    'SID': '\"USW2EC0FB4rqqc0WHfHgnRmXfTYyb\"',\n",
    "    '__cf_bm': '13OGb3qSxuDsMhFbttOSdWsA0Vl3ROm5l3o0wAZxqRc-1756067041-1.0.1.1-BgAgM73eyaEVsku9HsdCXgRcv5NOkGA3appsH43wffY30_zRwurcFeNtO_WJBgn9PUEGH_PPuwH4xuokzwDMFVVUX7N8JhvAYJ1ZWRahhV8',\n",
    "    # Adicione outros cookies do seu cURL se necessário\n",
    "}\n",
    "\n",
    "BASE_PAYLOAD = {\n",
    "    \"parentQid\": \"8e331f13-5696-41dd-8362-22da1d43bed7-017595e076\",\n",
    "    \"sortBy\": \"times-cited-descending\",\n",
    "    \"displayTimesCited\": \"true\",\n",
    "    \"displayCitedRefs\": \"true\",\n",
    "    \"product\": \"UA\",\n",
    "    \"colName\": \"WOS\",\n",
    "    \"fileOpt\": \"xls\",\n",
    "    \"action\": \"saveToExcel\",\n",
    "    \"view\": \"summary\",\n",
    "    \"isRefQuery\": \"false\",\n",
    "    \"locale\": \"en_US\",\n",
    "    \"filters\": \"fullRecord\"\n",
    "}\n",
    "\n",
    "# --- PARÂMETROS DE DOWNLOAD ---\n",
    "# Defina aqui quantos registros no total você quer baixar\n",
    "TOTAL_RECORDS_TO_DOWNLOAD = 12000\n",
    "PAGE_SIZE = 1000 # O Web of Science geralmente exporta em lotes de até 1000\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. FUNÇÕES AUXILIARES\n",
    "# ==============================================================================\n",
    "\n",
    "def download_data(total_records, page_size):\n",
    "    \"\"\"Baixa os arquivos de dados em lotes.\"\"\"\n",
    "    print(f\"🚀 Iniciando download de {total_records} registros em lotes de {page_size}...\")\n",
    "    downloaded_files = []\n",
    "    for i in range(0, total_records, page_size):\n",
    "        start_record = i + 1\n",
    "        end_record = i + page_size\n",
    "        if end_record > total_records:\n",
    "            end_record = total_records\n",
    "\n",
    "        print(f\"📄 Baixando registros de {start_record} a {end_record}...\")\n",
    "\n",
    "        payload = BASE_PAYLOAD.copy()\n",
    "        payload[\"markFrom\"] = str(start_record)\n",
    "        payload[\"markTo\"] = str(end_record)\n",
    "\n",
    "        try:\n",
    "            response = requests.post(URL, headers=HEADERS, cookies=COOKIES, json=payload, timeout=90)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            filename = f\"temp_export_{start_record}_to_{end_record}.xls\"\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "            downloaded_files.append(filename)\n",
    "            print(f\"   ✅ Arquivo '{filename}' salvo com sucesso.\")\n",
    "            time.sleep(3)  # Pausa de 3 segundos para não sobrecarregar o servidor\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Erro crítico durante o download: {e}\")\n",
    "            print(\"   O processo foi interrompido. Verifique sua conexão e se os cookies/headers são válidos.\")\n",
    "            return [] # Retorna lista vazia em caso de erro\n",
    "\n",
    "    return downloaded_files\n",
    "\n",
    "def combine_files(file_list):\n",
    "    \"\"\"Lê e combina múltiplos arquivos .xls em um único DataFrame.\"\"\"\n",
    "    print(\"\\n📊 Combinando arquivos em um único DataFrame...\")\n",
    "    all_dataframes = []\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            df = pd.read_excel(file)\n",
    "            all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Erro ao ler o arquivo {file}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        return None\n",
    "\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(\"   ✅ Arquivos combinados com sucesso!\")\n",
    "    return combined_df\n",
    "\n",
    "def cleanup_files(file_list):\n",
    "    \"\"\"Remove os arquivos temporários.\"\"\"\n",
    "    print(\"\\n🧹 Limpando arquivos temporários...\")\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"   - Arquivo '{file}' removido.\")\n",
    "        except OSError as e:\n",
    "            print(f\"   ⚠️ Erro ao remover o arquivo {file}: {e}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUÇÃO PRINCIPAL\n",
    "# ==============================================================================\n",
    "\n",
    "# Executa o download\n",
    "temp_files = download_data(TOTAL_RECORDS_TO_DOWNLOAD, PAGE_SIZE)\n",
    "\n",
    "# Se o download foi bem-sucedido, processa os dados\n",
    "if temp_files:\n",
    "    # Combina os arquivos em um DataFrame\n",
    "    dataset = combine_files(temp_files)\n",
    "\n",
    "    # Limpa os arquivos temporários\n",
    "    cleanup_files(temp_files)\n",
    "\n",
    "    if dataset is not None:\n",
    "        print(\"\\n🎉 Processo concluído! Os dados estão prontos na variável 'dataset'.\")\n",
    "        print(\"\\n--- Informações do DataFrame Final ---\")\n",
    "        dataset.info()\n",
    "        print(\"\\n--- 5 Primeiras Linhas do DataFrame ---\")\n",
    "        display(dataset.head())\n",
    "else:\n",
    "    print(\"\\n❌ Nenhuma coleta de dados foi realizada devido a um erro.\")\n"
   ],
   "id": "f13cefbb96eed977",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando download de 12000 registros em lotes de 1000...\n",
      "📄 Baixando registros de 1 a 1000...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 64\u001B[39m, in \u001B[36mdownload_data\u001B[39m\u001B[34m(total_records, page_size)\u001B[39m\n\u001B[32m     63\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m     response = requests.post(URL, headers=HEADERS, cookies=COOKIES, json=payload, timeout=\u001B[32m90\u001B[39m)\n\u001B[32m     66\u001B[39m     filename = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mtemp_export_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstart_record\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m_to_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mend_record\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.xls\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mNameError\u001B[39m: name 'requests' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 114\u001B[39m\n\u001B[32m    107\u001B[39m             \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m   ⚠️ Erro ao remover o arquivo \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m    109\u001B[39m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[32m    110\u001B[39m \u001B[38;5;66;03m# 3. EXECUÇÃO PRINCIPAL\u001B[39;00m\n\u001B[32m    111\u001B[39m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[32m    112\u001B[39m \n\u001B[32m    113\u001B[39m \u001B[38;5;66;03m# Executa o download\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m temp_files = download_data(TOTAL_RECORDS_TO_DOWNLOAD, PAGE_SIZE)\n\u001B[32m    116\u001B[39m \u001B[38;5;66;03m# Se o download foi bem-sucedido, processa os dados\u001B[39;00m\n\u001B[32m    117\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m temp_files:\n\u001B[32m    118\u001B[39m     \u001B[38;5;66;03m# Combina os arquivos em um DataFrame\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 74\u001B[39m, in \u001B[36mdownload_data\u001B[39m\u001B[34m(total_records, page_size)\u001B[39m\n\u001B[32m     71\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m   ✅ Arquivo \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilename\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m salvo com sucesso.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     72\u001B[39m     time.sleep(\u001B[32m3\u001B[39m)  \u001B[38;5;66;03m# Pausa de 3 segundos para não sobrecarregar o servidor\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m74\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m requests.exceptions.RequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     75\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m❌ Erro crítico durante o download: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     76\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m   O processo foi interrompido. Verifique sua conexão e se os cookies/headers são válidos.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'requests' is not defined"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
